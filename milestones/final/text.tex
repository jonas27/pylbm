
\chapter{Introduction}
The Lattice Boltzmann Method (LBM) is a numerical solution of (nonlinear) partial differential equations of the original BLT introduced in 1988 by McNamara and Zanetti~\cite{mcnamara1988boltzmann-method}.
It is used to simulate liquids\footnote{I will focus exclusively on liquids, but most aspects in the report hold for some gases as well.} in a closed system and is based on the core assumption that their flows can be approximated to particles on a lattice.
To simulate the correct flow of liquids a model must lead to the the Navier–Stokes equations in the macoscopic limit.
The Navier–Stokes equations are partial differential equations to describe the motion of viscous fluid substances, named after the physicists Claude-Louis Navier and George Gabriel Stokes.
It has been shown that the LBM does indeed fulfill these equations for incompressible subsonic flows of fluids.
Today, the LBM is used in a wide variety of fields from car aerodynamics to ocean current flows.

The LBM originates from the lattice gas automata (LGA) pioneered by Hardy, Pomeau and de Pazzis in the 1970s with the HPP-model~\cite{hardy1973timeHPP}.
This model could be used to simluate both gas and fluid flows, but did not not, as initially hoped by the authors, lead to the Navier-Stokes equation in the macroscopic limit.
Later lattice gas automata models like the FPH-model~\cite{PhysRevLett.56.1505-fhp} were able to satisfy the Navier-Stokes equation but were still plagued by many problens, like the lack of Galilean invariance~\cite{nie2008galileanInvariance} or the strong assumption
that each node is surrounded by discrete particle cells, which resulted in massive computing requirements.
It also assumed that streaming and collision happened synchronously for all nodes and thus the collision was non-deterministic.

In 1988 McNamara and Zanetti introduced the LBM as a direct alternative to the LGA~\cite{mcnamara1988boltzmann-method}.
Their new method "is based on the simulation of a very simple microscopic system, rather than on the direct integration of partial differential equations"~\cite{mcnamara1988boltzmann-method}.
Because of their close similarity, the LBM shares many features with the LGA, like the lack of Galilean invariance and both satisfy the Navier-Stokes equation in the macroscopic limit.
It, crucially, "directly stud[ies] the time evolution of the mean values"~\cite{mcnamara1988boltzmann-method} and thus does not need statistical averaging to compute the velocity, as is the case in LGA, leading to lower computing requirements.

The key points of the LBM success is its simplicity, relatively low consumption of computing requirements and easy parallelization of the algorithm.
This is achieved by approximating the fluid to particles on a grid and using a separate streaming and collision step to simulate the particles behaviour over time.
This is unlike other computational fluid dynamics (CFD) methods which directly solve the numerically macroscopic properties of a fluied, i.e. the mass, momentum, and energy.
Simulating the particles themselves also makes incorperating boundries and microscopic interactions easier than in most other CFD models.

The reminder of the report is structed in the follwing way: The second chapter introduce the theoretical background of the LBM. Chapter three introduces the most important implementations and chapter four shows the results of the milestones.
Afterwards I will conclude and give a future outlook.

\chapter{Theoretical Background}
The probability density function is used to describe the state of particles in a closed system and the Boltzmann transport equation tracks the time evolution of this function.
Combining these methods and discretizing them over a lattice leads to the LBM.
In this section I will discuss each individual component in greater detail.

\section{Probability Density Function}
The probability density function (PDF) describes the statistical probability of finding particles in a closed system not in equilibrium and is denoted by $f$ in the implementation.
It thereby replaces tagging each particle, as in molecular dynamic simulations.
The PDF is given by $f(r_i,v_i,t)$ where $r$ are the physical positions, $v$ the velocities and $t$ the time.
The probability for finding a particle in a certain part of the phase space is then given by equation~\ref{eq:pdf}.
\begin{equation}
  \label{eq:pdf}
  \begin{aligned}
    dP = f(r,v,t) d^{3}r d^{3}v
  \end{aligned}
\end{equation}
The phase space for equation \ref{eq:pdf} is given by $[r, r+dr, v, v+dv]$.
Thus, the probability for finding a particle in the phase space at position $r_i$ is only depended on the velocity $v_i$ and time $t$.

The probability of finding a single particle with an arbitraty place $r$ and an arbitrary velocity $v$ in the entire phase space is given by equation \ref{eq:prob-single}.
\begin{equation}
  \label{eq:prob-single}
  \begin{aligned}
    P = \int_{\Omega_{r}} \int_{\Omega_{\vec{v}}}  f(r,v,t) d^{3}r d^{3}v \quad \overset{!}{=} 1
  \end{aligned}
\end{equation}
Assuming the fluid is in a closed system where no particles are added or removed, it must hold that equation \href{eq:prob-single} is equal to one, i.e. normalized to unity. 
This means the chance to find an arbitrary particle in an arbitrary location is equal to one.

This then leads to the general form of the $i$-th moments of $ f(r,v,t) $ w.r.t. $ v $ shown in equation \ref{eq:general-moments}.
\begin{equation}
  \label{eq:general-moments}
  \begin{aligned}
    \mu_{i}(r) =\int_{\Omega_{v}}v^{i} f(r,v,t)d^{3}v
  \end{aligned}
\end{equation}
The first and second order moments are the main subjects of this project and can be readily interpreted.
The first order moment is the velocity in the system, which for liquids is the flow field.
In a 2D lattice a two dimensional vector for each field is needed to describe the average local velocity of each particle.
The second order moment is the kenetic energy density in the system which can be readily interpreted as the temperature of a fluid.
Different fluids can have different translation ratios between velocities and temperature and thus an increase in temperature in a closed system would lead to higher velocities but the exact increase is depended on the fluid.


\section{Boltzmann Transport Equation}
The Boltzmann transport equation (BTE) tracks the time evolution of the probability distribution function and was published by Ludwig Boltzmann in 1872. 
Ludwig Eduard Boltzmann (1844–1906) was an Austrian physicist whose greatest achievement was in the development of statistical mechanics~\cite{book}. 

To derive it we take the first order derivative of the PDF in respect to time as shown in
equation \ref{eq:BTE}.
\begin{equation}
  \label{eq:BTE}
  \begin{aligned}
    \frac{df}{dt} =\frac{\partial f}{\partial t} + \frac{dr(t)}{dt} \nabla_{r}f + \frac{dv(t)}{dt} \nabla_{v}f = \left( \frac{\partial f}{\partial t} \right)_{col}
  \end{aligned}
\end{equation}
Where the term $f$ is the PDF, $\frac{dr(t)}{dt}$ is called the velocity and $\frac{dv(t)}{dt}$ is called the acceleration and by Newtons law is the force acting on the particles devivded by their respective mass.
The \textit{l.h.s.} of equation \ref{eq:BTE} is called the streaming term and the \textit{r.h.s} is the collision term.
To implement the streaming and collision terms the BTE first needs to be discretized.
The implementation of the streaming and collision terms are discussed in more detail in chapter 3.


\section{Lattice-Boltzmann Method} \label{sec:lbm}
The BTE is defined in a continous phase space which is not readily implementable in computer code.
This can be overcome by approximating the continous phase space to a discrete phase space, called the Lattice-Boltzmann method (LBM).
The idea is to discretize over a lattice and use the partial derivatives to solve the BTE.
In this report the spatial dimension is discretized to a 2D lattice and the velocities are descritized to 9 directions also called a \textit{D2Q9}-model.
This scheme is illustarate in \ref{fig:d2q9-scheme} where $a$ shows the discretazation of the velocity space and $b$ the discretized of the physical space on a 2D grid with the velocity layered on top. Thus, each particle has a 2D positional vector and a 9D velocity vector.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\columnwidth]{d2q9_scheme.png}\\
  \small{(Material from lecture)}
  \caption[Discretization of the BTE]{Discretization of the BTE. a) is the discretization on the velocity space according to D2Q9 and b) the discretization of the physical space on a  2D lattice or grid.\\
  Source: Lecture}
  \label{fig:d2q9-scheme}
\end{figure}



\chapter{Implementation}
This chapter shows the most important code implementations, mainly the streaming, the collision, the boundaries and the moving top lid.
Throughout the report I will use the code notation $\_cxy$ where $c$ is the rolling dimension, here the velocity directions, and $x$ and $y$ are the dimensions of the physical space. The same logic applies for the local average velocities, notated as $\_axy$, where $a=2$, and the local densities $\_xy$.
The git repository for this report is under \href{https://github.com/jonas27/pylbm}{https://github.com/jonas27/pylbm} and can be installed with `\textit{pip install -e .}` from inside the root directory of the repo.

\section{Streaming}
The main part of milestone 1 is the streaming operator. It is explained here for reference and used throughout the report. 

From equation~\ref{eq:pdf} it follows that streaming and collision are two separate terms and collision can be ignored by setting it to zero, i.e. $\left( \frac{\partial f}{\partial t} \right)_{col} \overset{!}{=} 0$.
This simplifies the BTE to equation \ref{eq:streaming} 
\begin{equation}
  \label{eq:streaming}
  \begin{aligned}
    f_{i}(r+c_{i} \nabla t,t+\nabla t)=f_{i}(r,t)
  \end{aligned}
\end{equation}
and implies the movement of particles in the vacuum with no mutual interaction between particles.
The code implementation of a streaming function on a 2D lattice is shown in listing \ref{lst:streaming}
\begin{center}
\begin{lstlisting}[caption=Implementation of the streaming operator,label=lst:streaming, basicstyle=\small]
def stream(f_cxy: np.array) -> np.array:
    for i in range(1, 9):
        f_cxy[i, :, :] = np.roll(f_cxy[i, :, :], 
                      shift=C_CA[i], 
                      axis=(0, 1))
    return f_cxy
  \end{lstlisting}
\end{center}
$f_{cxy}$ is the PDF and $C \_ CA$ are the discretized velocity directions.
For each velocity direction the streaming shifts the velocities values in the respective direction.
Testing the streaming operator can be easily done by visualizing the different shift on a 2D grid for each velocity as shown in figure \ref{fig:m1-shifting} for the velocities $v=2$ $v=3$.
\begin{figure}[ht]
\centering
\resizebox{\columnwidth}{!}{\large\input{img/m2-streaming.pgf}}
\vspace*{-10mm}
\caption[Visualization of the streaming]{A visualization of the streaming operator for velocity 2 and 3. In the top row each element is shifted to the top. In the bottom  each element is shifted to the left.}
\label{fig:m1-shifting}
\end{figure}
The local densities and velocities are randomly initialized with an average of $\mu_{\rho}=\mu_{u}=0.5$ and a standard deviation of $std_{\rho}=std_{u}=0.01$ and used to calculate the equilibrium PDF.
For simplicity we only plot two velocity directions.
The top row in figure \ref{fig:m1-shifting} shows the streaming of each element to the top, i.e. using streaming direction $2$. In the bottom row each element is shifted to the left, which corresponds to the streaming in direction $3$.
This is the expected behaviour and concludes Milestone 1.

\section{Collision}
The collision term is the \textit{r.h.s} of the BTE \ref{eq:BTE} and represents the interaction between particles.
It is a complicated two particle scattering integral but can be approximated by a relaxation time approximation $\tau$ so that the PDF locally relaxes to an equilibrium distribution $f_{eq}(r,v,t)$.
The resulting discrete form of the BTE is shown in equation
\begin{equation}
  \label{eq:btw-discrete}
  \begin{aligned}
    f_{i}(r+ \nabla t c_{i},t+\nabla t)=f_{i}(r,t) + \omega \left( f_{i}^{eq}(r,t) - f_{i}(r,t) \right)
  \end{aligned}
\end{equation}
Because of the relaxation the PDF equilibrium is a local one and therefor depends on the local density $\rho$ and the local average velocity $u$.

The local density is just a summation over the velocities of the PDF, i.e. $\rho(r) = \sum_{i} f_{i}$.
An example implementation is shown in listing \ref{lst:rho}.
\begin{center}
\begin{lstlisting}[caption=Implementation of the local density,label=lst:rho, basicstyle=\small]
def local_density(f_cxy: np.array) -> np.array:
    r_xy = np.einsum("cxy -> xy", f_cxy)
    return r_xy
  \end{lstlisting}
\end{center}
The local average velocity is more complicated but effectively it is the summation over the velocity dimensions for each physical dimension divided by the local density.
It is calculated via $\textbf{u}(\textbf{r})=\frac{1}{\rho (\textbf{r})} \sum_{i} c_{i}f_{i}(\textbf{r})$ and respective code is shown in listing \ref{lst:vel}.
\begin{center}
  \begin{lstlisting}[caption=Implementation of the local average velocity.,label=lst:vel, basicstyle=\small]
def local_avg_velocity(f_cxy: np.array, r_xy: np.array):
    u_aij = np.einsum("ac, cxy->axy", C_CA.T, f_cxy) / r_xy
    return u_aij
  \end{lstlisting}
\end{center}
With this we can define the PDF equilibrium function as in equation \ref{eq:pdf-eq},
\begin{equation}
  \label{eq:pdf-eq}
  \begin{aligned}
    f_{i}^{eq} ( \rho , u ) = w_i \rho \left[ 1+3 c_i * u + \frac{9}{2}(c_i * u )^2 - \frac{3}{2} | u |^2 \right]
  \end{aligned}
\end{equation}
where $w_i$ for a D2Q9 lattice can be defined as follow: 
\begin{equation}
w_i = \left( \frac{4}{9}, \frac{1}{9}, \frac{1}{9}, \frac{1}{9}, \frac{1}{9}, \frac{1}{36}, \frac{1}{36}, \frac{1}{36}, \frac{1}{36} \right)
\end{equation}
It is important to note that the approximation in the collision term is not always accurate.
But under the assumption that all transport processes occur on a longer time scale it is appropriate to do.

\section{Boundaries}
The simplest boundaries are static walls. 
The bounce back from the walls are modeled to occur in the opposed direction. 
Listing \ref{lst:top} shows the code for top wall.
\begin{center}
  \begin{lstlisting}[caption=Bounce back at the top wall.,label=lst:top, basicstyle=\small]
def top_wall(f_cxy: np.array) -> np.array:
    f_cxy[4, :, -2] = f_cxy[2, :, -1]
    f_cxy[7, :, -2] = f_cxy[5, :, -1]
    f_cxy[8, :, -2] = f_cxy[6, :, -1]
    return f_cxy
  \end{lstlisting}
\end{center}
The wall reverses the effects of the streaming, just in the opposite direction.
For the bottom wall the velocity vectors would be reversed and instead of copying from the last row to the second last, the first row should be copied to the second column.
For the left and right wall, the left most (or right most) column should be copied to the second column with respect to the corresponding velocities.

Adding a velocity to the top wall leads to a bounce back with added or subtracted velocity as shown in listing \ref{lst:top-slide}. 
\begin{center}
  \begin{lstlisting}[caption=Bounce back at the top sliding wall.,label=lst:top-slide, basicstyle=\small]
def sliding_top_wall(f_cxy: np.array, velocity: float):
    r_top = ( f_cxy[[0, 1, 3], :, -2].sum(axis=0) + 
            2.0 * (f_cxy[2, :, -1] + f_cxy[5, :, -1] + f_cxy[6, :, -1]))
    f_cxy[4, :, -2] = f_cxy[2, :, -1]
    f_cxy[7, :, -2] = f_cxy[5, :, -1] - 6.0 * W_C[5] * r_top * velocity
    f_cxy[8, :, -2] = f_cxy[6, :, -1] + 6.0 * W_C[6] * r_top * velocity
    return f_cxy
  \end{lstlisting}
\end{center}
In the first line, the density at the sliding wall is calculated and later used in the update of the diagonal velocities. For other walls this could be implemented similarly, just with other velocities and row or columns depending on where the wall is located.

\section{Reynolds number}
The flow induced by a moving wall, in this report a moving lid, can be characterized by a certain Reynolds number (Re).
It expresses the ratio between the inertial terms and the viscous ones and can be calculated by equation \ref{eq:reynolds}.
\begin{equation}
    \nu = 1 / 3 * (1 / \omega - 1 / 2) \\
\end{equation}
\begin{equation}\label{eq:reynolds}
     Re = \frac{top-vel * L_x}{\nu}
\end{equation}


\section{Parallelization}
To run the code in an high performance computing environment, e.g. super computers, the code must be able to be used in parallel.
This means the different operations inside the LBM have to be able to be splitted up and calculated by different processes. 
To achieve this dry nodes are added around each splitted up section of the grid.
\begin{figure}[ht]
\centering
\includegraphics[width=0.5\columnwidth]{milestones/final/img/latttice-ghost-beschriftet.png}
\caption[Wet and dry nodes]{$3\times3$ grids where in blue we have the wet nodes and in gray are the dry nodes.}
\label{fig:para-ghost}
\end{figure}
Basically, two additional rows and columns are added to each sub grid. 
The nodes inside them are called dry nodes, because they are not part of the actual simulation but are solely used for computational purposes.


\chapter{Results}
In this chapter I will present the results obtained for the milestones. I chose to keep the order of the milestones and name them chapters accordingly.
In all experiments the lattice size includes dry nodes and the wet nodes.

\section{M3: Shear Wave Decay}
Shear wave decay describes the amplitude decay of shear waves, i.e. elastic waves travelling through the body of an object.
An interesting aspect is the effects of a certain $\omega$, and thereby the kinematic viscosity $\nu(\omega)$ of a fluid, on the shear wave decay.
The kinematic viscosity is especially relevant in fluid dynamics and is defined as the ratio of the dynamic viscosity $\mu$ over the density $\rho$ of the fluid as in equation \ref{eq:viscocity}.
\begin{equation}\label{eq:viscocity}.
\nu = \frac{\mu}{\rho}
\end{equation}
This milestone is separated in three different parts. 
The general motion is to set a sinusoidal density or velocity
perturbation and measure how fast it decays, i.e. the equilibrium state is reached.
For each part I use the following setup:
\begin{enumerate}
    \item $T = 3000$
    % \item $\mu_{\rho}=\mu_{u}=0.5$
    \item $\epsilon = 0.1$
    \item $L_{x}$, $L_{y}$ = 30, 30
\end{enumerate}
The code for this milestone can be found \href{https://github.com/jonas27/pylbm/tree/master/milestones/m3}{here}.

\paragraph{Density decay} 
In a closed fluid system with an unequal density, the density is expected to decay over time. In the LBM this decay is dependent on the collision frequency $\omega$ and the size of the system, where both parameters are expected to be inversely correlated to the time it takes to reach the equilibrium state.

To see if the simulation behaves according to the theoretical expectations, I simulate a sine wave in the densities over the length (x dimension) of the physical space.
Specifically, the initial density distribution is $\rho (\textbf{r},t_{0})=\rho_{0}+\epsilon \sin \left( \frac{2\pi x}{L_{x}} \right)$, where $L_{x}$ is the length of the domain in the x direction, and the initial local average velocity is set to zero, i.e. $\textbf{u}(\textbf{r},0)=0$.
I set $\rho_{0}=0.5$ and use three omega values for the simulation: $\omega=[0.5,1.0,1.7]$.
The density decay for these values is shown in \ref{fig:m3-1}.
\begin{figure}[ht]
\centering
\resizebox{\columnwidth}{!}{\large\input{img/m3-1-density-decay.pgf}}
\vspace*{-10mm}
\caption[Density decay]{The density decay for $\omega=[0.5,1.0,1.7]$. The less viscose the fluid the faster the amplitude decays.}
\label{fig:m3-1}
\end{figure}
It shows that all curves start at the same amplitude at $t=0$ but that the smaller $\omega$ the faster the density decays which is inline with expectations. 
For $t\rightarrow \infty$ the density amplitude of all three curves converges towards $0$ and stabilizes around $\rho_{0}$.
The higher $\omega$, i.e. the collision frequency, the more the initial sine wave is able to propagate through the system. Conversely, lowering $\omega$ increases the kinematic viscosity of the simulated fluid and decreasing the time it takes for the system to get into the equilibrium state.

% Because $\omega = \frac{1}{\tau}$

\paragraph{Velocity decay}
Similarly, the velocity in an imbalanced state is expected to converge towards the equilibrium.
To test this, an initial distribution of $\rho_{0}=1$ $u_{y}(r,0)=0$ and $u_{x}(r,t_{0})=\rho_{0}+\epsilon \sin \left( \frac{2\pi x}{L_{y}} \right)$ was chosen, where $L_{y}$ is the length of the domain in the y direction, i.e. the height.
\begin{figure}[ht]
\centering
\resizebox{\columnwidth}{!}{\large\input{img/m3-2-velocity-decay.pgf}}
\vspace*{-10mm}
\caption[Velocity decay]{The velocity decay for $\omega=[0.3,1.1,1.7]$. The more viscose the fluid the faster the amplitude decays.}
\label{fig:m3-2-vel}
\end{figure}
The velocity decays much slower with higher $\omega$ compared to smaller values, i.e. a positive correlation between collision frequency and the time to reach the equilibrium.
Similarly to the density decay, this is in line with expectations and shows that the simulation is working.

The comparison between the numerical decay of the initial perturbation to the theoretical decay is shown in \ref{fig:m3-2-norm-vel} for $0.1 \leq \omega \leq 1.9 $.
\begin{figure}[ht]
\centering
\resizebox{0.9\columnwidth}{!}{\large\input{img/m3-2-shear-wave-decay.pgf}}
% \vspace*{-10mm}
\caption[Normalized density decay]{The density decay for $0.1 \leq \omega \leq 1.9 $ and $T=3,000$. a) is the numerical decay and b) is the theoretical decay.}
\label{fig:m3-2-norm-vel}
\end{figure}
Both graphs in figure \ref{fig:m3-2-norm-vel} use the same axes, time on the x axis and the normalized maximum velocity values along the y dimension (height dimension) on the y axis.
It illustrates that the higher the collision frequency the faster the system converges towards an equilibrium state where $a)$ is the numerical, velocity decay and $b)$ is the theoretical viscosity decay.
For all values of $\omega$, except for $\omega=0.1$ and $\omega=0.3$, the numerical and the theoretical decay are very similar
In addition, figure \ref{fig:m3-2-vel} also shows that increases in the collision frequency $\omega$ have a disproportional bigger effect on the time it takes to reach the equilibrium state. However, the effect gets lower the bigger the lattice size.


\paragraph{Kinematic viscosity scaling with $\omega$}
The kinematic viscosity is (theoretically) only dependent on $\omega$. 
Plotting the theoretical and the numerical kinematic viscosities next to each other, as done in figure \ref{fig:m3-3-kinematic-viscocity}, should then reflect the model performs according to theory. 
\begin{figure}[ht]
\centering
\resizebox{0.8\columnwidth}{!}{\large\input{img/m3-3-kinematic-viscocity-scaling.pgf}}
% \vspace*{-10mm}
\caption[Kinematic viscosity]{The numerical and theoretical kinematic viscosity for $0.1 \leq \omega \leq 1.9 $. For low $\omega$ the kinematic viscocities diverge.}
\label{fig:m3-3-kinematic-viscocity}
\end{figure}
For low values of $\omega$ the theoretical and numerical values of $\nu$ diverge quite significantly, while for values $\geq\omega$ the values are almost identical.

The reason for the divergence is the relative low physical space which leads to inaccuracies for high viscocities. Increasing the physical space to a $300\times300$ lattice alleviates the issue as shown in figure \ref{fig:m3-3-update}.
\begin{figure}[ht]
\centering
\resizebox{\columnwidth}{!}{\large\input{img/m3-3-update.pgf}}
% \vspace*{-10mm}
\caption[Kinematic viscosity]{The numerical and theoretical velocity amplitude decay and respective kinematic viscosity for $\omega=[0.1, 0.3] $.}
\label{fig:m3-3-update}
\end{figure}

Initially, I wanted to only include the correct numerical results, but decided to include the numerical incorrect results and then show that in a bigger lattice the problem does not occur.
I found it interesting that the correctness of the results would be influenced by the collision frequency \textbf{and} the dimensions of the physical space. 
As $\omega$ only influences how much the equilibrium function is integrated in the collision term, I suspect that the increased rolling over the boundaries somehow causes the equilibrium function to be more imprecise. 
This is just one possible reason, but no definitive explanation.

\section{M4: Couette flow}
The Couette flow is a fluid flow in between two reflective boundaries, where one of the two boundaries is moving relative to the other. 
This introduces a constant perturbation which means the system can never reach an equilibrium state.
The Couette flow can be used to model the the Earth's mantle and atmosphere and results in a Couette solution, where the velocity in the moving boundary direction is almost constant and decreasing from the moving boundary to the (relative) static one.

To model such a behaviour I use the following parameters:
\begin{enumerate}
    \item $L_{x}, L_{y} = 100, 100$
    \item $\rho(\textbf{r}, 0) = 0$
    \item $\textbf{u}(\textbf{r}, 0) = 0$
    \item $T = 100000$
    \item $top\_lid\_vel = 1$
    \item $\omega = 1$
\end{enumerate}
The corresponding results in the github repo can be found  \href{https://github.com/jonas27/pylbm/blob/master/milestones/m4/m4.ipynb}{here}. Unfortunately I could not provide the raw \textit{velocities.npy} data to reconstruct the results as they are too big. 

Figure \ref{fig:m4-1-streamplot} shows the Couette flow results in a \textit{streamplot} and \textit{imshow}.
\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{milestones/final/img/m4-1-streamplot.png}
\vspace*{-4mm}
\caption[Couette flow ]{Streamplot of the Couette flow after $t=200$, $t=400$, $t=600$ showing the velocity in the x direction on the grid. At the right end of the figure is an imshow of the velocities at $t=100,000$.}
\label{fig:m4-1-streamplot}
\end{figure}
After $t=200$ more than half the grid has a directional flow. At $t=400$ it is almost $90\%$ and at $t=600$ all particles flow in the x direction. 
After $100,000$ time steps the velocity reaches a state where the fluid flow stays (almost) the same, but no equilibrium state is reached, i.e. $f^{eq}_{cxy}\neq f_{cxy}$. 
It is decreasing from the top to bottom, with $v_{x}(y=1)=1$ and $v_{x}(y=98)=0$. 
The first and last rows are dry nodes.

To visualize the velocities over time I plot a single column over time on the lattice on the left side of figure \ref{fig:m4-1-velocities-over-time}.
\begin{figure}[ht]
\centering
\resizebox{\columnwidth}{!}{\large\input{img/m4-1-velocities-over-time.pgf}}
\caption[Column velocities over time]{The left graph shows the velocities in $x$ direction over time. The right graph shows the numerical solution of $v_{x}$ at $t=100,000$ vs the cleaned solution.}
\label{fig:m4-1-velocities-over-time}
\end{figure}
At first the velocities are high only at the moving lid, but over time the velocities inside the lattice are increasing as well.
Finally, the velocities converge to a straight line where the velocity is constantly decreasing from the moving wall with velocity $v_{lid}=0.995$ to the opposite wall with velocity $v_{lid}=0.005$
However, figure \ref{fig:m4-1-velocities-over-time} only shows the wet nodes.
Plotting the dry nodes as well, as done on the right hand side of figure \ref{fig:m4-1-velocities-over-time}, shows that the dry nodes do not follow this pattern. 
The dry nodes are influenced from the dry nodes on the other side via the streaming and the velocity is applied to the wet nodes but leaks into the dry nodes.
Thus, the top dry nodes are too slow and the bottom dry nodes are too fast.
However, fitting a curve to the wet nodes and extending it on the dry nodes, as done in the orange line in figure \ref{fig:m4-1-velocities-over-time}, shows that the cleaned values behave as expected.

\section{M5: Poiseuille Flow}
The Poiseuille flow describes a pressure-induced fluid flow between two opposite and static boundaries. 
It is most commonly used to model the flow of a fluid through a pipe. 
Because the outer boundaries introduce friction, the velocity is expected to be highest in the middle of the pipe on lowest at the sides.
To model this flow I use the following parameters:
\begin{enumerate}
    \item $L_{x}, L_{y} = 100, 80$
    \item $\rho(\textbf{r}, 0) = 1$
    \item $\textbf{u}(\textbf{r}, 0) = 0$
    \item $T = 100,000$
    \item $omega = 0.5$
    \item $\rho_{in} = 1.01$
    \item $\rho_{out} = 0.99$
\end{enumerate}
The corresponding results in the github repo can be found  \href{https://github.com/jonas27/pylbm/blob/master/milestones/m5/m5.ipynb}{here}.

At the inlet a constant pressure is applied and propagated through the grid until it reaches the outlet. The resulting pressure difference between the in- and outlet is $0.02$. 
The graphical results are shown in figure \ref{fig:m5-1-vel-time}.
\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{milestones/final/img/m5-1-vel-time.png}
\vspace*{-4mm}
\caption[Poiseuille Velocities]{Poiseuille velocities visualized with streamplot at $t=50$ and $t=100$ and with imshow at $t=500$ and $t=10,000$ }
\label{fig:m5-1-vel-time}
\end{figure}
The two left grids visualize the velocities with \textit{streamplot} at $t=50$ and $t=100$ while the last two grids use \textit{imshow}. 

At $t=50$ and $t=100$ the velocities in the \textit{streamplot} flow in the correct direction and the effects of the reflective boundaries are clearly visible at the bottom and top of the pipe. While in the middle of the pipe the arrows point parallel to the boundaries, close to the boundaries turbulences caused by friction develop, which become less pronounced over time.
At $t=500$ and $t=10,000$ I use an \textit{imshow} to show the magnitudes of the velocities.
At $t=500$ they are still very small, between $0\leq\textbf{u}_{x}(x,y)\leq0.3$, but become larger and converge towards $0\leq\textbf{u}_{x}(x,y)\leq1.1$.

Finally, I compare the theoretical solution to the numerical solution at $t=100,000$ shown in figure \ref{fig:m5-1-num-theo}.
\begin{figure}[ht]
\centering
\resizebox{0.6\columnwidth}{!}{\large\input{img/m5-1-num-theo.pgf}}
\caption[Poiseuille numerical vs analytical]{Poiseuille velocities comparing the numerical (blue) and theoretical (red) solution at $t=100,000$.}
\label{fig:m5-1-num-theo}
\end{figure}
The theoretical solution, red line, has similar velocities at the boundaries but significantly higher velocities at the boundaries (the exact difference is $0.00397$).
A possible explanation could be the velocity reduction because of the collisions.

To test this hypothesis I first calculated the velocities for $L_x=L_y=30$ and $L_x=L_y=300$ as shown in the first two graphs of figure \ref{fig:m5-1-num-theo-extended}.
\begin{figure}[ht]
\centering
\resizebox{\columnwidth}{!}{\large\input{img/m5-1-num-theo-extended.pgf}}
\caption[Poiseuille analysis extended]{Poiseuille velocities comparing the numerical (blue) and theoretical (red) solution at $t=100,000$ for different pipe sizes. The maximal difference following the order of the graphs is: $-0.00148$, $0.161374$ and $0.000348$, respectively.}
\label{fig:m5-1-num-theo-extended}
\end{figure}
Interestingly, the difference in the theoretical and the numerical velocities increases with the size of the grid or pipe. In a small pipe, the numerical solution is actually higher than the theoretical, whereas at $L=300$ the difference between expected and actual velocity is $0.161$. 
I then tried to find the numerical solution most similar to the theoretical solution, which c.p. is around $L=50$ (right graph in figure \ref{fig:m5-1-num-theo-extended} with a difference of maximal $0.000348$.


\section{M6: Sliding Lid}
The sliding lid problem has been developed to benchmark the efficieny of algorithms simulating the flow of viscous (mostly) incompressible fluid flow~\cite{bruneau20062-lid-cavity}.
It has been investigated by many authors starting in the early 80ties and a Reynolds number of $1,000$ has been widely adopted to make the results comparable~\cite{bruneau20062-lid-cavity}.

In the lecture the the following parameters were recommended:
\begin{enumerate}
    \item $L_x = L_y = 300$
    \item $\rho(\textbf{r},0) = 1.0$
    \item $\textbf{u}(\textbf{r},0) = 0.0$
    \item $T = 100000$
    \item $\omega = 1.7$
    \item $top-vel = 0.1$
\end{enumerate}
The corresponding results in the github repo can be found  \href{https://github.com/jonas27/pylbm/blob/master/milestones/m6/m6.ipynb}{here}.
These parameters result in a Reynolds number of $1,020$, which could be close enough to be comparable with similar research.

Running this configuration results in a lid driven cavity  after $T=100,000$ shown in figure \ref{fig:m6-1}.
The most striking feature in the graphs is the moving cavity. Because of the moving lid, it forms in the top right corner and steadily moves to the center of the graph. 
Cavities also form in the bottom left and right corners, albeit a lot smaller ones. 

\clearpage
\begin{figure}[ht]
\centering
% \begin{frame}{}
\textbf{\large a)}
% \animategraphics[loop, width=\columnwidth]{10}{ani/ani-}{0}{119}
\animategraphics[loop,autoplay, width=\columnwidth]{10}{ani/ani-}{0}{119}
\vspace*{2mm}

\textbf{\large b)}
\includegraphics[width=\columnwidth]{milestones/final/img/m6-1-vels.png}
\caption[Lid-driven cavity]{Lid-driven cavity shown as \textit{streamplot}. a) is an animation of the lid-driven cavity for time steps of $1,000$ viewable in some pdf readers (tested in Adobe and Okular) and can be downloaded \href{https://raw.githubusercontent.com/jonas27/pylbm/master/milestones/m6/m6.gif}{here}. b) shows the same solution but as static graphs in time steps of $10,000$. }
\label{fig:m6-1}
\end{figure}
\clearpage



\section{M7: Parallelization}
Parallelization is the concept of running multiple processes at the same time and thereby enables high performance computing on multiple nodes and CPUs. 
But its important to understand that parallelization is only useful with efficient concurrency. 
Concurrency is a program's ability to run a program at different time points and/or on different processes and is the foundation of parallelization.
For example, a program running the same problem multiple times, using only the fastest result and discarding the rest is using parallelization but very inefficiently.
On the other hand, splitting up the calculations in multiple smaller parts, executing them in different processes and combining them in the end can decrease the run time significantly.
To make use of parallelization we use the Message Passing Interface (MPI) framework using the openMPI~\cite{gabriel04:_open_mpi} implementation which has an effective implementation of concurrency.

\say{Performance evaluation is an effective and inexpensive method for assessing research results}~\cite{beyer2019-benchmark}.
It is critical in areas like high-performance computing, but can often be influenced by outside factors.  
To measure the implementation performance I let the code run on the BwUniCluster for the following configurations:
\begin{enumerate}
    \item 2 Nodes with $[2, 8, 18, 32^{*},]$ CPUs
    \item 4 Nodes with $[1, 4, 9, 16^{*}, 25, 36]$ CPUs
    \item Slurm default memory configuration
\end{enumerate}
This results in a total of eight overlapping configurations and two configurations only possible for 4 nodes. Each total number of CPUs is present twice, once on two and once on four nodes. 
This allows me to compare the performance between setups with more nodes but less CPUs per node and less nodes but more CPUs per node.
The configurations marked with $*$ do not result in a perfect geometric splitting of the LBM grid but are included to provide one more datapoint to compare the performance between 2 and 4 nodes.
Running on one, six or eight nodes was not possible on the BwUniCluster.

To recreate the results, run the following command from listing \ref{m7run} from inside the root directory of the repo.
\begin{lstlisting}[language=bash, label=m7run, caption=Run lbm with extra arguments.]
  $ python3 pylbm/lbm.py -d --x_dim=300 --y_dim=300  \
            --velocity=0.0 --time=100000 --density=1.0 \
            --omega=1.7 --lid_vel=0.1
\end{lstlisting}
\textit{-d} starts in debug mode and will print the current configurations and print after 1,000 time steps.
Alternatively, to recreate the results for all CPU configurations one can simply run the bash file \textit{schedule.sh} under \url{https://github.com/jonas27/pylbm/blob/master/milestones/m7/schedule.sh}.
This will start a slurm job for each configuration.

Finally, GPUs have recently gained popularity~\cite{boyer2013gpu-pop} for two main purposes: training artificial neural networks (ANN) and mining crypto currency. 
Both of these rely on heavy number crunching and GPUs are well suited for that purpose without going further into the details of GPUs\footnote{For the interested reader this article provides more explanation \url{https://bit.ly/3zV04Ik}}
\footnote{TPUs are designed around tensor cores which allow for faster matrix multiplication and are even more specialized then GPUs.}.
I rewrote the code using Pytorch~\cite{pytorch} and benchmarked the application on a \textit{Tesla V100 SXM2} with 32GB of memory and 125 Tensor TFLOPS cores.
I use double precision floating-point (FP64) as is the case in the numpy implementation.
But differences in how the GPU uses mathematical approximations could lead to tiny differences in the results~\cite{precision-nvidia}.
The Pytorch implementation can be found \href{https://github.com/jonas27/pylbm/blob/master/pylbm/torchlbm/lbm.py}{here}.


The results for the parallization results are shown in figure \ref{fig:m7}.
The MLUPS score is calculated by the number of grid points multiplied by the number of time steps and divided by the runtime.
Contrary to my expectations using more nodes was superior to less nodes with the same number of total CPUs. 
Less nodes would also mean less communication outside the node which should theoretically speed up the computation. The contrary was the case.

It is also surprising that the GPU performed as well as 4 nodes and 16 CPUs per node, so 64 CPUs in total. 
Both run for 87 seconds and achieved an MLUPS of $103448275$.
The GPU I used has 125 TPUs and uses advanced algorithm to make matrix computation even faster. This means comparing the results between CPU and GPU is very hard and needs deep knowledge of both architectures, the underlying network stack and the frameworks.
\begin{figure}[ht]
\centering
\textbf{\large a)} \\
\resizebox{0.7\columnwidth}{!}{\large\input{img/m7.pgf}}
\vspace*{2mm}

\textbf{\large b)} \\
\resizebox{0.7\columnwidth}{!}{\large\input{img/m7-mlups.pgf}}
\caption[Performance per configuration.]{The performance per configuration. The blue line is the performance for 2 nodes, the orange line is the performance for 4 nodes and the red dot is the performance for the GPU. The GPU is marked at 125 CPUs but has instead 125 TPUs.
a) shows the performance in seconds whereas b) shows the performance in MLUPS.}
\label{fig:m7}
\end{figure}



\chapter{Conclusion}
The LBM is an easy to use method to model fluid and gas flows. In this report I first explained the evolution and theoretical background of the LBM. 
Afterwards, I showed and explained important implementations like the streaming and collision functions.
Finally, all relevant milestones were achieved and analysed.

For the future, using MPI on GPUs and TPUs should make the computation even faster.
Using CuPy could make the integration with BwUniCluster quite easy and it would be super cool to try. Unfortunately I was not able to do it, but it could speed up the computation quite significantly.
However, this is given that MPI on GPUs behaves similarly to CPU, which is probably not the case, as GPUs pack hundreds of cores and are not designed to run in parallel. 
This is changing making the future more interesting than ever.



